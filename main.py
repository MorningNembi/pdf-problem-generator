import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_teddynote.prompts import load_prompt
from langchain_core.prompts import loading
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote import logging
from dotenv import load_dotenv
import glob
import yaml
import os

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

logging.langsmith("[Project] PDF RAG")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(".cache/files", exist_ok=True)
os.makedirs(".cache/embeddings", exist_ok=True)

st.title("PDF ê¸°ë°˜ ë¬¸ì œ ìƒì„±ê¸° QAğŸ’¬")

if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "retriever" not in st.session_state:
    st.session_state["retriever"] = None
if "chain_mc" not in st.session_state:
    st.session_state["chain_mc"] = None
if "chain_desc" not in st.session_state:
    st.session_state["chain_desc"] = None

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    # ê°ê´€ì‹, ì„œìˆ í˜• í”„ë¡¬í”„íŠ¸ YAML íŒŒì¼ ì„ íƒ
    mc_prompt_files = glob.glob("prompts/multiple_choice_*.yaml")
    desc_prompt_files = glob.glob("prompts/descriptive_*.yaml")
    selected_prompt_mc = st.selectbox("ê°ê´€ì‹ í”„ë¡¬í”„íŠ¸ ì„ íƒ", mc_prompt_files, index=0)
    selected_prompt_desc = st.selectbox(
        "ì„œìˆ í˜• í”„ë¡¬í”„íŠ¸ ì„ íƒ", desc_prompt_files, index=0
    )
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])
    selected_model = st.selectbox(
        "LLM ì„ íƒ", ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"], index=0
    )
    generate_btn = st.button("ë¬¸ì œ ìƒì„±")


# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥ í•¨ìˆ˜
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ ì¶”ê°€ í•¨ìˆ˜
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# PDF íŒŒì¼ ì„ë² ë”© ìƒì„± (ìºì‹±)
@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    # ë¬¸ì„œ ë¡œë“œ
    loader = PDFPlumberLoader(file_path)
    docs = loader.load()

    # ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    split_documents = text_splitter.split_documents(docs)

    # ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


# ì²´ì¸ ìƒì„± í•¨ìˆ˜ (í”„ë¡¬í”„íŠ¸ YAML íŒŒì¼ì„ ë°›ì•„ ì²´ì¸ ìƒì„±)
def create_chain(retriever, prompt_path, model_name="gpt-4o"):
    with open(prompt_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    prompt = loading.load_prompt_from_config(config)
    llm = ChatOpenAI(model_name=model_name, temperature=0)
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain


# íŒŒì¼ ì—…ë¡œë“œ ì‹œ retriever ë° ì²´ì¸ ìƒì„±
if uploaded_file:
    retriever = embed_file(uploaded_file)
    st.session_state["retriever"] = retriever
    st.session_state["chain_mc"] = create_chain(
        retriever, selected_prompt_mc, model_name=selected_model
    )
    st.session_state["chain_desc"] = create_chain(
        retriever, selected_prompt_desc, model_name=selected_model
    )

# ì´ˆê¸°í™” ë²„íŠ¼
if clear_btn:
    st.session_state["messages"] = []

print_messages()

# "ë¬¸ì œ ìƒì„±" ë²„íŠ¼ í´ë¦­ ì‹œ, ê°ê´€ì‹ 3ë¬¸ì œì™€ ì„œìˆ í˜• 2ë¬¸ì œ ìƒì„±
# "ë¬¸ì œ ìƒì„±" ë²„íŠ¼ í´ë¦­ ì‹œ, ê°ê´€ì‹ 3ë¬¸ì œì™€ ì„œìˆ í˜• 2ë¬¸ì œë¥¼ ìƒì„±í•˜ë„ë¡ ìˆ˜ì •
if generate_btn:
    if st.session_state["retriever"] is None:
        st.error("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")
    else:
        questions_output = []
        # ê°ê´€ì‹ ë¬¸ì œ 3ê°œ ìƒì„±
        for i in range(3):
            user_query = f"ê°ê´€ì‹ ë¬¸ì œ {i+1} ìƒì„±"
            # .invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ ì‹¤í–‰ (ë˜ëŠ” chain(user_query)ë„ ê°€ëŠ¥)
            response = st.session_state["chain_mc"].invoke(user_query)
            questions_output.append(f"**ê°ê´€ì‹ ë¬¸ì œ {i+1}:**\n{response}")
        # ì„œìˆ í˜• ë¬¸ì œ 2ê°œ ìƒì„±
        for i in range(2):
            user_query = f"ì„œìˆ í˜• ë¬¸ì œ {i+1} ìƒì„±"
            response = st.session_state["chain_desc"].invoke(user_query)
            questions_output.append(f"**ì„œìˆ í˜• ë¬¸ì œ {i+1}:**\n{response}")

        st.write("### ìƒì„±ëœ ë¬¸ì œ")
        for q in questions_output:
            st.write(q)


# ê¸°ì¡´ ì±„íŒ… ì…ë ¥ (ì¶”ê°€ ì§ˆë¬¸ìš©)
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
if user_input:
    st.chat_message("user").write(user_input)
    # ê¸°ë³¸ì ìœ¼ë¡œ ê°ê´€ì‹ ì²´ì¸ì„ ì‚¬ìš©í•´ ë‹µë³€(í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
    response = st.session_state["chain_mc"].stream(user_input)
    ai_answer = ""
    container = st.empty()
    for token in response:
        ai_answer += token
        container.markdown(ai_answer)
    add_message("user", user_input)
    add_message("assistant", ai_answer)
